#How to run

cd Exercise1/Problem1

One directory: 

hadoop jar ../path/to/Problem1.jar HadoopWordCount ../enwiki-articles/AA ../resultCount

hadoop jar ../path/to/Problem1.jar HadoopWordPairs ../enwiki-articles/AA ../resultPairs

hadoop jar ../path/to/Problem1.jar HadoopWordStripes ../enwiki-articles/AA ../resultStripes


(d):

hadoop jar ../path/to/Problem1.jar HadoopWordCount ../enwiki-articles/AA ../enwiki-articles/AB ../enwiki-articles/AC ../enwiki-articles/AD ../resultCount

hadoop jar ../path/to/Problem1.jar HadoopWordPairs ../enwiki-articles/AA ./enwiki-articles/AB ../enwiki-articles/AC ../enwiki-articles/AD ../resultPairs

hadoop jar ../path/to/Problem1.jar HadoopWordStripes ../enwiki-articles/AA ./enwiki-articles/AB ../enwiki-articles/AC ../enwiki-articles/AD ../resultStripes



#Results

(c)

Time computer (AA directory):
	Count: (end: 27.30, start: 27.07) 23 seconds
	Pairs: (end: 34.37, start: 33.26) 71 seconds
	Stripes: (end: 52.06, start: 50.45) 81 seconds

Time computer (AA/AB directories):
	Count: (end: 28.48, start: 28.04) 44 seconds
	Pairs: (end: 39.36, start: 37.19) 137 seconds
	Stripes: (end: 55.25, start: 52.41) 164 seconds
	
Time computer (AA/AB/AC directories):
	Count: (end: 30.32, start: 29.24) 68 seconds
	Pairs: (end: 43.45, start: 40.19) 206 seconds
	Stripes: (end: 00.27, start: 56.21) 246 seconds

Time computer (AA/AB/AC/AD directories):
	Count: (end: 32.41, start: 31.13) 88 seconds
	Pairs: (end: 49.33, start: 44.57) 276 seconds
	Stripes: (end: 11.10, start: 01.32) 578 seconds

(d)

Time computer (m=1):
	Pairs: (end: 12.49, start: 12.22) 27 seconds
	Stripes: (end: 14.36, start: 13.52) 44 seconds

Time computer (m=2):
	Pairs: (end: 15.47, start: 15.07) 40 seconds
	Stripes: (end: 17.01, start: 16.03) 58 seconds

Time computer (m=3):
	Pairs: (end: 18.54, start: 18.02) 52 seconds
	Stripes: (end: 20.22, start: 19.16) 66 seconds

Time computer (m=4):
	Pairs: (end: 22.00, start: 20.58) 62 seconds
	Stripes: (end: 26.19, start: 25.00) 79 seconds

Time computer (m=5):
	Pairs: (end: 34.37, start: 33.26) 71 seconds
	Stripes: (end: 52.06, start: 50.45) 81 seconds

Time computer (m=10):
	Pairs: (end: 29.04, start: 27.15) 109 seconds
	Stripes: (end: 31.33, start: 29.27) 126 seconds


#Code Explanations:

(a)

HadoopWordCount, HadoopWordPairs, HadoopWordStripes:

In map function we added two different regex for numbers and words:

String patternNumber = "^(\\d+)((\\.)(\\d+))?$"; 
String patterWord = "^([a-z]|[\\_]|[\\-]+)([\\-]|[\\_]|[a-z])*$"; 

Then we used java Pattern and Matcher library to analyze the text value; in case we found that the value is not a number, we used the lowercase function on the word.

Reduce function is literally the same as before.

For the division in two different output file we used a partitioner which simply checks if they key is a number or a word. If we found a number then we use the first reduce task, otherwise the second one.

We added "job.setNumReduceTasks(2);" to the code in order to obtain to different reduce task (one for numbers and the other for words).

To obtain the file sorted in descending order we added another job that use a WritableComparator with IntWritable to sort the number of occurrences.

(b)

In order to add a max distance 'm' we used "for loop" in both HadoopWordPairs and HadoopWordStripes; After each value, we check if it is a value of the same group (word or number) and then we add the pair to the context. We also check if "j < splitLine.length" ("k+i<splitLine.length" in HadoopWordStripes, respectively) to be sure that we do not obtain a OutOfBound Error.

(c)

We added in all the source code something like this:

FileInputFormat.setInputPaths(job, new Path(args[0]));
for(int i = 1; i<args.length-1;i++) {
	FileInputFormat.addInputPath(job, new Path(args[i]));
}
FileOutputFormat.setOutputPath(job, new Path(args[args.length-1]));

To take more directories as input.

The results are in #Results section.

(d)

The results are in #Results section.



#QUESTIONS:
1) Which observation can you make regarding the runtime of your programs between the steps described
in (c) and (d)?

Times are directly proportional to the number of folders (c) and to the number of near words to find (d). In particular, it is easy to see that there is a linear correlation (experiment (c)) for HadoopWordCount and HadoopWordPairs in all four cases, instead for HadoopWordStripes only for the first three.
As far as experiment (d) is concerned, there is no particular correlation between the times.

2) What is the principal difference between these two scalability tests in terms of “linear scalability”?

(c) for count and pairs it seems to be linearly scalable, because the time with all 4 directories is 4 times the execution time with one directory. As for stripes, on the other hand, the results are very different from 3 to 4 folders. We can suppose several motivations for this, like the overhead due to the number of words or that stripes does not have linearly scalability, but that it has a greater complexity.
(d) from the point of view of scalability, we can say that as regards the execution times the number of neighbors is not a factor on which linear scalability can be applied, because the number of neighbors that can be obtained are completely different and depend a lot from the text.
